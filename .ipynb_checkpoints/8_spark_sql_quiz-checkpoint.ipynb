{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling with Spark SQL Quiz\n",
    "\n",
    "This quiz uses the same dataset and most of the same questions from the earlier \"Quiz - Data Wrangling with Data Frames Jupyter Notebook.\" For this quiz, however, use Spark SQL instead of Spark Data Frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Wrangling Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "path = \"data/sparkify_log_small.json\"\n",
    "user_log = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.createOrReplaceTempView(\"user_log_tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Which page did user id \"\"(empty string) NOT visit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "allemptypages = spark.sql(\"\"\"\n",
    "          SELECT DISTINCT\n",
    "          page\n",
    "          FROM \n",
    "          user_log_tab  \n",
    "          WHERE userId = ''\n",
    "          ORDER BY 1\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpages = spark.sql(\"\"\"\n",
    "                     SELECT DISTINCT\n",
    "                     page\n",
    "                     from user_log_tab\n",
    "                     ORDER BY 1\n",
    "                     \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Downgrade', 'Error', 'Logout', 'NextSong', 'Save Settings', 'Settings', 'Submit Downgrade', 'Submit Upgrade', 'Upgrade']\n"
     ]
    }
   ],
   "source": [
    "NonVisitedPages = []\n",
    "for row in set(allpages.collect()) - set(allemptypages.collect()):\n",
    "    NonVisitedPages.append(row.page)\n",
    "print(sorted(NonVisitedPages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 - Reflect\n",
    "\n",
    "Why might you prefer to use SQL over data frames? Why might you prefer data frames over SQL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "How many female users do we have in the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT userId)|\n",
      "+----------------------+\n",
      "|                   462|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT\n",
    "          COUNT(DISTINCT userId)\n",
    "          FROM\n",
    "          user_log_tab\n",
    "          WHERE gender = 'F'\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "How many songs were played from the most played artist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|  artist|count(1)|\n",
      "+--------+--------+\n",
      "|Coldplay|      83|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT\n",
    "          artist,\n",
    "          count(*)\n",
    "          FROM\n",
    "          user_log_tab\n",
    "          WHERE page = \"NextSong\"\n",
    "          GROUP BY artist\n",
    "          ORDER BY 2 DESC\n",
    "          LIMIT 1\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 (challenge)\n",
    "\n",
    "How many songs do users listen to on average between visiting our home page? Please round your answer to the closest integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(ishome)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"ishome\", lambda ishome: int(ishome == \"Home\"), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedf = spark.sql(\"\"\"\n",
    "          SELECT \n",
    "          userId, \n",
    "          page, \n",
    "          ts,\n",
    "          CASE WHEN page = 'Home' THEN 1 ELSE 0 END as ishome,\n",
    "          SUM(CASE WHEN page = 'Home' THEN 1 ELSE 0 END) OVER (PARTITION BY userId ORDER BY ts DESC) AS ishome_cumsum\n",
    "          FROM\n",
    "          user_log_tab\n",
    "          WHERE page IN ('NextSong', 'Home')\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedf.createOrReplaceTempView(\"basetab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_avg_df = spark.sql(\"\"\"\n",
    "          SELECT\n",
    "          userId,\n",
    "          ishome_cumsum,\n",
    "          COUNT(*) AS Avg_Song_No\n",
    "          FROM\n",
    "          basetab\n",
    "          WHERE page = \"NextSong\"\n",
    "          GROUP BY \n",
    "          userId, \n",
    "          ishome_cumsum\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-----------+\n",
      "|userId|ishome_cumsum|Avg_Song_No|\n",
      "+------+-------------+-----------+\n",
      "|   100|            1|          1|\n",
      "|   100|            2|          2|\n",
      "|   100|            3|          2|\n",
      "+------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_avg_df.filter(F.col(\"userId\") == 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_avg_df.createOrReplaceTempView(\"user_avg_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|Total_Avg_Song_No|\n",
      "+-----------------+\n",
      "|6.898347107438017|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT\n",
    "          AVG(Avg_Song_No) AS Total_Avg_Song_No\n",
    "          FROM\n",
    "          user_avg_tab\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
